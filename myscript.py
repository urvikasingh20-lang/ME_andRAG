# -*- coding: utf-8 -*-
"""ME_andRAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tlnOFYs2qksvJi28WeAxndY6ovExNTuT
"""

!pip uninstall -y langchain langchain-core langchain-community langchain-text-splitters langchain-classic requests

!pip install \
  langchain==0.3.27 \
  langchain-core==0.3.75 \
  langchain-community==0.3.16 \
  langchain-text-splitters==0.3.9 \
  sentence-transformers \
  faiss-cpu \
  transformers \
  torch \
  requests==2.32.4

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document

from transformers import pipeline

print("âœ… Everything is finally working")

"""1. You ask a question
2. System searches your document
3. Finds most relevant parts
4. Adds them to a prompt
5. Sends that to the model

"""

from langchain_community.document_loaders import TextLoader

loader = TextLoader("ABOUT_ME.txt")
docs = loader.load()

print(docs[0].page_content[:200])

"""ENCODING-EMBEDIINGMODEL(MINILM)<CONTEXT BASED EMBEDDINGS OF DOC"""

splitter=RecursiveCharacterTextSplitter(chunk_size=200,chunk_overlap=20)
chunks=splitter.split_documents(docs)

embeddings=HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
db=FAISS.from_documents(chunks,embeddings)

"""DECODING-GPT2 MODEL <TEXT GENERATION BASED ON DOC(RAG)COMAPRISION"""

generator=pipeline("text-generation",
    model="Qwen/Qwen2.5-0.5B-Instruct")

BOT_STYLE = """
You are a helpful and emotionally intelligent assistant.

RULES:
-ONLY ANSWER THE QUESTIONS DO NOT EXPLAIN YOUR RESONING
-ID YOU DONT KNOW ABOUT ANYTHING JUST SAY I DONT KNOW .
-ALWAYS USE FRIENDLY TONE OR AS YOUR USER SOUNDS .
-USE CONTEXT IF SOMETHING IS SIMILAR LIKE BEHAVIOUR, ACADEMC OTHERWISE YOU ARE A FRIENDLY CHATBOT TRYING TO COMMUNICATE EASILY WITH THE THE USER.
-DO NOT HALLUCINATE.
- NEVER repeat the context.
- NEVER invent user background.
- If the user is emotional, respond warmly.
- Answer in plain text only
"""

"""" What am I learning? "

        â†“
Embedding model encodes it

        â†“
FAISS compares with stored vectors

        â†“
Most similar chunks returned

"""

def rag_chat(query):
    docs = db.similarity_search(query, k=2)
    context = "\n".join([d.page_content for d in docs]) if docs else ""

    prompt = f"""
{BOT_STYLE}

<CONTEXT>
{context}
</CONTEXT>

<USER_QUESTION>
{query}
</USER_QUESTION>

<ASSISTANT_ANSWER>
"""

    output = generator(
        prompt,
        max_new_tokens=150,
        do_sample=True,
        temperature=0.8,
        repetition_penalty=1.3,
        no_repeat_ngram_size=3,
        return_full_text=False     # âœ… ADD THIS


    )[0]["generated_text"]


    # Keep only first clean reply
    for stop_word in ["```", "<", "System:", "Context:", "The answer should"]:
        if stop_word in output:
            response = output.split(stop_word)[0]

    return response.strip()

print("Chatbot is ready. Type 'exit' to stop.\n")

while True:
    query = input("You: ")

    if query.lower() == "exit":
        print("Bot: Bye ðŸ‘‹")
        break

    response = rag_chat(query)
    print("\nBot:", response, "\n")

